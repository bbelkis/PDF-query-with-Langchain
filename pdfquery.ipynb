{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF query with Langchain and AstraDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Notebook\n",
    "\n",
    "This notebook follows a systematic process for document retrieval and question answering using the Langchain library. The workflow can be summarized as follows:\n",
    "\n",
    "### Document Processing:\n",
    "\n",
    "1/ Read the PDF Document:\n",
    "Utilizes PyPDF2 to read the content of a PDF document.\n",
    "\n",
    "2/ Chunk the Text:\n",
    "Breaks down the extracted text into manageable chunks, employing Langchain's text splitting mechanism. This helps control token size and optimize processing efficiency.\n",
    "\n",
    "3/ Embed the Chunks:\n",
    "Applies embedding techniques, specifically using the OpenAI language model, to convert each text chunk into a vector representation. This step captures the semantic meaning of the text for further analysis.\n",
    "\n",
    "4/ Add Embeddings to Vector Database:\n",
    "Incorporates the embedded chunks into a Langchain vector database, leveraging AstraDB for efficient storage and retrieval. This ensures organized and scalable management of the vectorized text data.\n",
    "\n",
    "### Query Process:\n",
    "\n",
    "1/ Embed the Question:\n",
    "Embeds the user's input question using the same language model, creating a vector representation that captures the question's semantic content.\n",
    "\n",
    "2/ Search for Top K Similar Embeddings:\n",
    "Queries the vector database to identify the top K similar embeddings to the embedded question. This similarity search is crucial for finding the most relevant information based on the semantic context of the query.\n",
    "\n",
    "3/ Output the Answer with Top K Similarities:\n",
    "Presents the answer to the user by utilizing the embeddings associated with the top K similar documents. This approach ensures that the response is based on the most relevant information, as determined by the similarity scores of the vectorized data.\n",
    "\n",
    "In essence, the notebook seamlessly combines document processing with vectorization and retrieval techniques to provide a robust and interactive question-answering system. The utilization of Langchain components and integration with AstraDB contribute to the efficiency and scalability of the overall workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Langchain componenets\n",
    "from langchain.vectorstores.cassandra import Cassandra\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "# providing the AstraDB integration with langchain\n",
    "import cassio\n",
    "# read the pdfs\n",
    "from PyPDF2 import PdfReader\n",
    "# load env vars\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASTRA_DB_APPLICATION_TOKEN = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
    "ASTRA_DB_ID = os.getenv(\"ASTRA_DB_ID\")\n",
    "OPENAI_API_KEY= os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfreader = PdfReader(\"attention_is_all_you_need.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate\n",
    "# extract all text from pdf\n",
    "raw_text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content= page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conncetion with the AstraDB\n",
    "cassio.init(token= ASTRA_DB_APPLICATION_TOKEN, database_id= ASTRA_DB_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n",
      "/home/vscode/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(openai_api_key = OPENAI_API_KEY)\n",
    "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Langchain Vector Store\n",
    "astra_vector_store = Cassandra(\n",
    "    embedding= embedding,\n",
    "    table_name=\"qa_pdf\",\n",
    "    session= None,\n",
    "    keyspace=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# Split text into chuncks to not increase the token size\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=200,\n",
    "    length_function= len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 67 headlines\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset into the vector store\n",
    "astra_vector_store.add_texts(texts)\n",
    "print(\"Inserted %i headlines\" %len(texts))\n",
    "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Answer: Transformers are transduction models that rely entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs or convolution. They have an encoder-decoder structure and use self-attention to generate continuous representations of input sequences, which are then used by the decoder to generate an output sequence one element at a time. \n",
      " First documents by relevance:\n",
      "[0.90] \\ To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entir\n",
      "[0.90] \\ To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entir\n",
      "[0.90] \\ To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entir\n",
      "\\Answer: An attention mechanism is a method used in sequence modeling and transduction tasks to allow the model to focus on relevant parts of the input or output sequence, regardless of their distance. This is achieved by assigning weights to different parts of the sequence, indicating their relevance. The model then uses these weights to give more attention to important parts of the sequence while processing it. In most cases, attention mechanisms are used in conjunction with recurrent networks, but the Transformer model architecture proposed in this work relies entirely on attention mechanisms, allowing for more efficient parallelization and potentially better performance. \n",
      " First documents by relevance:\n",
      "[0.92] \\ Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion\n",
      "[0.92] \\ Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion\n",
      "[0.92] \\ Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query_text= input(\"\\nEnter your question (or type 'quit' to exit):\")\n",
    "    if query_text.lower() ==\"quit\":\n",
    "        break\n",
    "    answer = astra_vector_index.query(query_text, llm=llm).strip()\n",
    "    print(\"\\Answer: %s \" %answer)\n",
    "    print(\" First documents by relevance:\")\n",
    "    for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=3):\n",
    "        print(\"[%0.2f] \\ %s\" %(score, doc.page_content[:100]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
